{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left-wing vs Right-wing Media on US President Donald Trump\n",
    "\n",
    "This notebook primarily focuses on data collection and formatting.\n",
    "\n",
    "## News Data Acquisition\n",
    "\n",
    "There are no readily available datasets which give right-wing and left-wing media data. So, we have to extract it from sources on the web. Now, from popular sources, it is widely accepted that CNN is left-leaning and Fox News is right-leaning. Therefore, we will use their content as proxies for left-leaning and right-leaning discussion on Donald Trump and the incumbent government.\n",
    "\n",
    "### Data from CNN and Fox\n",
    "\n",
    "Both CNN and Fox News have a search feature, where we will search for Donald Trump, and extract URLs of the news article from there. The idea is to get around 3000-4000 articles per set. So, we will extract 3000-4000 URLs from the search options and then extract title and text from each article's URL. This is done using Selenium and BeautifulSoup. Selenium has been used to open the search browser and extract the URLs. BeautifulSoup can extract the text from each news article URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN URLs Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "\n",
    "chrome_path = r\"C:\\Users\\Yash Kumar\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "cnn_masterlist = []\n",
    "\n",
    "try:\n",
    "    for counter in range(1, 600): # Focusing on ~600 pages of search results\n",
    "        driver =  webdriver.Chrome(chrome_path)\n",
    "\n",
    "        if counter == 1:\n",
    "            driver.get(\"https://www.cnn.com/search/?q=donald%20trump&size=10&page=1&sort=newest&type=article\")\n",
    "            time.sleep(5) # inducing delay counter of 5 seconds to prevent DOS attack\n",
    "\n",
    "        else:\n",
    "            num = (counter - 1) * 10\n",
    "            driver.get(f\"\"\"https://www.cnn.com/search/?q=donald%20trump&size=10&page={counter}&from={num}&sort=newest&type=article\"\"\")\n",
    "            time.sleep(5)\n",
    "            # Every search page increments num by 10 and counter by 1\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            # Below given is the URL xpath\n",
    "            post = driver.find_element_by_xpath(f\"/html/body/div[5]/div[2]/div/div[2]/div[2]/div/div[3]/div[{i}]/div[2]/h3/a\")\n",
    "            link = post.get_attribute(\"href\")\n",
    "            cnn_masterlist.append(link)\n",
    "            \n",
    "        driver.close()\n",
    "        \n",
    "        if counter % 10 == 0:\n",
    "            print(counter, time.time()-a)\n",
    "\n",
    "except:\n",
    "    print(\"worked until page\", str(counter))\n",
    "\n",
    "print(time.time() - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fox URLs Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "\n",
    "chrome_path = r\"C:\\Users\\Yash Kumar\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "fox_masterlist = []\n",
    "\n",
    "try:\n",
    "    for counter in range(1, 600):\n",
    "        driver =  webdriver.Chrome(chrome_path)\n",
    "\n",
    "        if counter == 1:\n",
    "            driver.get(\"\"\"https://www.foxnews.com/search-results/search?q=donald%20trump&ss=fn&type=story&start=0\"\"\")\n",
    "            time.sleep(3) # inducing delay counter of 5 seconds to prevent DOS attack\n",
    "\n",
    "        else:\n",
    "            num = (counter - 1) * 10\n",
    "            driver.get(f\"\"\"https://www.foxnews.com/search-results/search?q=donald%20trump&ss=fn&type=story&start={num}\"\"\")\n",
    "\n",
    "        for i in range(3, 13):\n",
    "            post = driver.find_element_by_xpath(f\"\"\"//*[@id=\"search-container\"]/div[{i}]/div/div/h3/a\"\"\")\n",
    "            # xpath of Fox URLs\n",
    "            link = post.get_attribute(\"href\")\n",
    "\n",
    "            if fox_masterlist == []:\n",
    "                fox_masterlist.append(link)\n",
    "\n",
    "            elif fox_masterlist[-1] != link:\n",
    "                # Lots of repititive links in Fox news in searches, mostly one after the other\n",
    "                fox_masterlist.append(link)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, time.time() - a)\n",
    "\n",
    "except:\n",
    "    print(\"worked until page\", str(counter))\n",
    "\n",
    "print(time.time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving URLs as data frames\n",
    "CNN_trump = pd.DataFrame({'CNN_URL':cnn_masterlist})\n",
    "CNN_trump.to_csv(\"CNN URLs.csv\")\n",
    "\n",
    "Fox_trump = pd.DataFrame({'Fox_URL':fox_masterlist})\n",
    "Fox_trump.to_csv(\"Fox URLs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Title and text scraper from URL\n",
    "def cnn_text_title(url):\n",
    "    try:\n",
    "        time.sleep(0.2)\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        # HTML tag associated with paragraphs\n",
    "        name_box = soup.find_all(\"div\", attrs={\"class\": lambda L: L and L.startswith(\"zn-body__paragraph\")})\n",
    "        text = \" \".join(chk.get_text(' ', strip=True) for chk in name_box) # Concatenating all paragraphs toogether\n",
    "        return [text, soup.find(\"h1\").text] # Title\n",
    "    \n",
    "    except:\n",
    "        return [\"\", \"\"]\n",
    "    \n",
    "CNN_trump[\"text_title\"] = CNN_trump[\"CNN_URL\"].apply(lambda x: cnn_text_title(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Title and text scraper from URL\n",
    "def fox_text_title(url):\n",
    "    try:\n",
    "        time.sleep(0.2)\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        # HTML tag associated with paragraphs\n",
    "        name_box = soup.find_all(\"p\", attrs={\"class\":''})\n",
    "        text = \" \".join(chk.get_text(' ', strip=True) for chk in name_box if len(list(chk.descendants)) == 1).replace(\"\\xa0\", \" \").replace(\"&apos;\", \"\")\n",
    "        return [text, soup.find(\"h1\").text] # Title\n",
    "    except:\n",
    "        return [\"\", \"\"]\n",
    "    \n",
    "Fox_trump[\"text_title\"] = Fox_trump[\"Fox_URL\"].apply(lambda x: fox_text_title(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting title and text from list of text and title\n",
    "CNN_trump[\"title\"] = CNN_trump[\"text_title\"].apply(lambda x: x[1])\n",
    "CNN_trump[\"text\"] = CNN_trump[\"text_title\"].apply(lambda x: x[0])\n",
    "\n",
    "Fox_trump[\"title\"] = Fox_trump[\"text_title\"].apply(lambda x: \"\" if x == \"\" else x[1])\n",
    "Fox_trump[\"text\"] = Fox_trump[\"text_title\"].apply(lambda x: \"\" if x == \"\" else x[0])\n",
    "\n",
    "# CNN_trump.drop(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been converted to a pandas dataframe with URL, Title and Text features. These dataframes have been saved as pickled files as they are easier to handle than CSV files, which are easy to get corrupted with text data due to presence of extra commas, spaces, tabs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CNN_trump.drop(\"text_title\", axis=1).to_pickle(\"CNN Trump Articles.pkl\")\n",
    "Fox_trump.drop(\"text_title\", axis=1).to_pickle(\"Fox Trump Articles.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
